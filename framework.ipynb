{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4embtkV0pNxM"
   },
   "source": [
    "# Deep Learning\n",
    "\n",
    "## preprocessing training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "input_file_dir = \"/Users/ninglee/Documents/IntrutionDection/datasets\"\n",
    "train_file_name = \"kddcup.data_10_percent.txt\"\n",
    "test_file_name = \"corrected.txt\"\n",
    "header_file_name = \"header.txt\"\n",
    "train_files = os.path.join(input_file_dir, train_file_name)\n",
    "test_files = os.path.join(input_file_dir, test_file_name)\n",
    "header_files = os.path.join(input_file_dir, header_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(header_files, 'r') as f:\n",
    "    header = f.readline().strip().split(',')\n",
    "train_dataset = pd.read_csv(train_files)\n",
    "test_dataset = pd.read_csv(test_files)\n",
    "train_dataset.columns = header\n",
    "test_dataset.columns = header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "494020 311028\n"
     ]
    }
   ],
   "source": [
    "train_dataset_size = train_dataset.shape[0]\n",
    "test_dataset_size = test_dataset.shape[0]\n",
    "train_dataset = pd.concat([train_dataset, test_dataset], axis=0)\n",
    "print train_dataset_size, test_dataset_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def labels_map(label):\n",
    "    label = str(label).split('.')[0]\n",
    "    if label == 'normal':\n",
    "        return 0\n",
    "    if label in ['ipsweep', 'mscan', 'nmap', 'portsweep', 'saint', 'satan']: #PROBE\n",
    "        return 1\n",
    "    if label in ['apache2', 'back', 'land', 'mailbomb', 'neptune', 'pod', 'processtable', 'smurf', 'teardrop', 'udpstorm']: #DOS\n",
    "        return 2\n",
    "    if label in ['buffer_overflow', 'httptunnel', 'loadmodule', 'perl', 'ps', 'rootkit', 'sqlattack', 'xterm']: #U2R\n",
    "        return 3\n",
    "    if label in ['ftp_write', 'guess_passwd', 'imap', 'multihop', 'named', 'phf', 'sendmail', 'snmpgetattack', 'snmpguess', 'spy', 'warezclient', 'warezmaster', 'worm', 'xlock', 'xsnoop']: #R2L\n",
    "        return 4\n",
    "    \n",
    "train_dataset['labels'] = train_dataset['labels'].apply(labels_map)\n",
    "labels_dummies = pd.get_dummies(train_dataset['labels'], prefix='label')\n",
    "train_dataset = pd.concat([train_dataset,labels_dummies], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>duration</th>\n",
       "      <th>protocol_type</th>\n",
       "      <th>service</th>\n",
       "      <th>flag</th>\n",
       "      <th>src_bytes</th>\n",
       "      <th>dst_bytes</th>\n",
       "      <th>land</th>\n",
       "      <th>wrong_fragment</th>\n",
       "      <th>urgent</th>\n",
       "      <th>hot</th>\n",
       "      <th>...</th>\n",
       "      <th>dst_host_serror_rate</th>\n",
       "      <th>dst_host_srv_serror_rate</th>\n",
       "      <th>dst_host_rerror_rate</th>\n",
       "      <th>dst_host_srv_rerror_rate</th>\n",
       "      <th>labels</th>\n",
       "      <th>label_0</th>\n",
       "      <th>label_1</th>\n",
       "      <th>label_2</th>\n",
       "      <th>label_3</th>\n",
       "      <th>label_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>tcp</td>\n",
       "      <td>http</td>\n",
       "      <td>SF</td>\n",
       "      <td>239</td>\n",
       "      <td>486</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>tcp</td>\n",
       "      <td>http</td>\n",
       "      <td>SF</td>\n",
       "      <td>235</td>\n",
       "      <td>1337</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>tcp</td>\n",
       "      <td>http</td>\n",
       "      <td>SF</td>\n",
       "      <td>219</td>\n",
       "      <td>1337</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>tcp</td>\n",
       "      <td>http</td>\n",
       "      <td>SF</td>\n",
       "      <td>217</td>\n",
       "      <td>2032</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>tcp</td>\n",
       "      <td>http</td>\n",
       "      <td>SF</td>\n",
       "      <td>217</td>\n",
       "      <td>2032</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 47 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   duration protocol_type service flag  src_bytes  dst_bytes  land  \\\n",
       "0         0           tcp    http   SF        239        486     0   \n",
       "1         0           tcp    http   SF        235       1337     0   \n",
       "2         0           tcp    http   SF        219       1337     0   \n",
       "3         0           tcp    http   SF        217       2032     0   \n",
       "4         0           tcp    http   SF        217       2032     0   \n",
       "\n",
       "   wrong_fragment  urgent  hot   ...     dst_host_serror_rate  \\\n",
       "0               0       0    0   ...                      0.0   \n",
       "1               0       0    0   ...                      0.0   \n",
       "2               0       0    0   ...                      0.0   \n",
       "3               0       0    0   ...                      0.0   \n",
       "4               0       0    0   ...                      0.0   \n",
       "\n",
       "   dst_host_srv_serror_rate  dst_host_rerror_rate  dst_host_srv_rerror_rate  \\\n",
       "0                       0.0                   0.0                       0.0   \n",
       "1                       0.0                   0.0                       0.0   \n",
       "2                       0.0                   0.0                       0.0   \n",
       "3                       0.0                   0.0                       0.0   \n",
       "4                       0.0                   0.0                       0.0   \n",
       "\n",
       "   labels  label_0  label_1  label_2  label_3  label_4  \n",
       "0       0        1        0        0        0        0  \n",
       "1       0        1        0        0        0        0  \n",
       "2       0        1        0        0        0        0  \n",
       "3       0        1        0        0        0        0  \n",
       "4       0        1        0        0        0        0  \n",
       "\n",
       "[5 rows x 47 columns]"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "protocal_type_dummies = pd.get_dummies(train_dataset.protocol_type, prefix='protocol_type')\n",
    "service_dummies = pd.get_dummies(train_dataset.service, prefix='service')\n",
    "flag_dummies = pd.get_dummies(train_dataset.flag, prefix='flag')\n",
    "train_dataset = pd.concat([train_dataset, protocal_type_dummies, service_dummies, flag_dummies], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max1 = train_dataset.src_bytes.max(); min1 = train_dataset.src_bytes.min();\n",
    "max2 = train_dataset.dst_bytes.max(); min2 = train_dataset.dst_bytes.min();\n",
    "train_dataset['src_bytes_norm'] = (train_dataset.src_bytes - min1) / float(max1 - min1)\n",
    "train_dataset['dst_bytes_norm'] = (train_dataset.dst_bytes - min2) / float(max2 - min2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.drop(['protocol_type', 'service', 'flag', 'src_bytes', 'dst_bytes','labels'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.astype('float')\n",
    "# train_dataset = (train_dataset - train_dataset.min()) / (train_dataset.max() - train_dataset.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocessing test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub_train_dataset = train_dataset.iloc[train_dataset_size:, :].sample(n=50000)\n",
    "sub_test_dataset = train_dataset.iloc[:train_dataset_size, :].sample(n=10000)\n",
    "sub_train_labels = sub_train_dataset[['label_0', 'label_1', 'label_2', 'label_3', 'label_4']]\n",
    "sub_test_labels = sub_test_dataset[['label_0', 'label_1', 'label_2', 'label_3', 'label_4']]\n",
    "sub_train_dataset.drop(['label_0', 'label_1', 'label_2', 'label_3', 'label_4'], axis=1, inplace=True)\n",
    "sub_test_dataset.drop(['label_0', 'label_1', 'label_2', 'label_3', 'label_4'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_dataset = train_dataset.iloc[train_dataset_size:,:]\n",
    "train_dataset = train_dataset.iloc[:train_dataset_size, :]\n",
    "train_labels = train_dataset[['label_0', 'label_1', 'label_2', 'label_3', 'label_4']]\n",
    "test_labels = test_dataset[['label_0', 'label_1', 'label_2', 'label_3', 'label_4']]\n",
    "train_dataset.drop(['label_0', 'label_1', 'label_2', 'label_3', 'label_4'], axis=1, inplace=True)\n",
    "test_dataset.drop(['label_0', 'label_1', 'label_2', 'label_3', 'label_4'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(494020, 119) (494020, 5)\n",
      "(311028, 119) (311028, 5)\n"
     ]
    }
   ],
   "source": [
    "print train_dataset.shape, train_labels.shape\n",
    "print test_dataset.shape, test_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 119) (50000, 5)\n",
      "(10000, 119) (10000, 5)\n"
     ]
    }
   ],
   "source": [
    "print sub_train_dataset.shape, sub_train_labels.shape\n",
    "print sub_test_dataset.shape, sub_test_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>duration</th>\n",
       "      <th>land</th>\n",
       "      <th>wrong_fragment</th>\n",
       "      <th>urgent</th>\n",
       "      <th>hot</th>\n",
       "      <th>num_failed_logins</th>\n",
       "      <th>logged_in</th>\n",
       "      <th>num_compromised</th>\n",
       "      <th>root_shell</th>\n",
       "      <th>su_attempted</th>\n",
       "      <th>...</th>\n",
       "      <th>flag_RSTOS0</th>\n",
       "      <th>flag_RSTR</th>\n",
       "      <th>flag_S0</th>\n",
       "      <th>flag_S1</th>\n",
       "      <th>flag_S2</th>\n",
       "      <th>flag_S3</th>\n",
       "      <th>flag_SF</th>\n",
       "      <th>flag_SH</th>\n",
       "      <th>src_bytes_norm</th>\n",
       "      <th>dst_bytes_norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.00000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.00000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>50000.0</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>5.000000e+04</td>\n",
       "      <td>50000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>19.496860</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000640</td>\n",
       "      <td>0.00006</td>\n",
       "      <td>0.017200</td>\n",
       "      <td>0.002560</td>\n",
       "      <td>0.17576</td>\n",
       "      <td>0.008120</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002700</td>\n",
       "      <td>0.058080</td>\n",
       "      <td>0.000160</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000860</td>\n",
       "      <td>0.800780</td>\n",
       "      <td>0.000160</td>\n",
       "      <td>2.360905e-06</td>\n",
       "      <td>0.000136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>475.794869</td>\n",
       "      <td>0.004472</td>\n",
       "      <td>0.038466</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>0.529707</td>\n",
       "      <td>0.051706</td>\n",
       "      <td>0.38062</td>\n",
       "      <td>0.751895</td>\n",
       "      <td>0.014141</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.051892</td>\n",
       "      <td>0.233897</td>\n",
       "      <td>0.012648</td>\n",
       "      <td>0.006324</td>\n",
       "      <td>0.029313</td>\n",
       "      <td>0.399418</td>\n",
       "      <td>0.012648</td>\n",
       "      <td>3.775728e-05</td>\n",
       "      <td>0.002167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.514331e-07</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.499542e-07</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.488371e-06</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>53771.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.00000</td>\n",
       "      <td>101.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>165.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.605842e-03</td>\n",
       "      <td>0.359027</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 119 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           duration          land  wrong_fragment       urgent           hot  \\\n",
       "count  50000.000000  50000.000000    50000.000000  50000.00000  50000.000000   \n",
       "mean      19.496860      0.000020        0.000640      0.00006      0.017200   \n",
       "std      475.794869      0.004472        0.038466      0.01000      0.529707   \n",
       "min        0.000000      0.000000        0.000000      0.00000      0.000000   \n",
       "25%        0.000000      0.000000        0.000000      0.00000      0.000000   \n",
       "50%        0.000000      0.000000        0.000000      0.00000      0.000000   \n",
       "75%        0.000000      0.000000        0.000000      0.00000      0.000000   \n",
       "max    53771.000000      1.000000        3.000000      2.00000    101.000000   \n",
       "\n",
       "       num_failed_logins    logged_in  num_compromised    root_shell  \\\n",
       "count       50000.000000  50000.00000     50000.000000  50000.000000   \n",
       "mean            0.002560      0.17576         0.008120      0.000200   \n",
       "std             0.051706      0.38062         0.751895      0.014141   \n",
       "min             0.000000      0.00000         0.000000      0.000000   \n",
       "25%             0.000000      0.00000         0.000000      0.000000   \n",
       "50%             0.000000      0.00000         0.000000      0.000000   \n",
       "75%             0.000000      0.00000         0.000000      0.000000   \n",
       "max             3.000000      1.00000       165.000000      1.000000   \n",
       "\n",
       "       su_attempted       ...        flag_RSTOS0     flag_RSTR       flag_S0  \\\n",
       "count       50000.0       ...            50000.0  50000.000000  50000.000000   \n",
       "mean            0.0       ...                0.0      0.002700      0.058080   \n",
       "std             0.0       ...                0.0      0.051892      0.233897   \n",
       "min             0.0       ...                0.0      0.000000      0.000000   \n",
       "25%             0.0       ...                0.0      0.000000      0.000000   \n",
       "50%             0.0       ...                0.0      0.000000      0.000000   \n",
       "75%             0.0       ...                0.0      0.000000      0.000000   \n",
       "max             0.0       ...                0.0      1.000000      1.000000   \n",
       "\n",
       "            flag_S1       flag_S2       flag_S3       flag_SF       flag_SH  \\\n",
       "count  50000.000000  50000.000000  50000.000000  50000.000000  50000.000000   \n",
       "mean       0.000160      0.000040      0.000860      0.800780      0.000160   \n",
       "std        0.012648      0.006324      0.029313      0.399418      0.012648   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.000000      0.000000      0.000000      1.000000      0.000000   \n",
       "50%        0.000000      0.000000      0.000000      1.000000      0.000000   \n",
       "75%        0.000000      0.000000      0.000000      1.000000      0.000000   \n",
       "max        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "       src_bytes_norm  dst_bytes_norm  \n",
       "count    5.000000e+04    50000.000000  \n",
       "mean     2.360905e-06        0.000136  \n",
       "std      3.775728e-05        0.002167  \n",
       "min      0.000000e+00        0.000000  \n",
       "25%      1.514331e-07        0.000000  \n",
       "50%      7.499542e-07        0.000000  \n",
       "75%      1.488371e-06        0.000000  \n",
       "max      5.605842e-03        0.359027  \n",
       "\n",
       "[8 rows x 119 columns]"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_train_dataset.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model1: MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 200\n",
    "feature_size = 119\n",
    "num_labels = 5\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, feature_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "#   tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(sub_test_dataset.values[:,:])\n",
    "  \n",
    "  # Variables.\n",
    "  hidden_weights = tf.Variable(\n",
    "    tf.truncated_normal([feature_size, 1024]))\n",
    "  output_weights = tf.Variable(\n",
    "    tf.truncated_normal([1024, num_labels]))\n",
    "  weights = [hidden_weights, output_weights]\n",
    "  hidden_biases = tf.Variable(tf.zeros([1024]))\n",
    "  output_biases = tf.Variable(tf.zeros([num_labels]))\n",
    "  biases = [hidden_biases, output_biases]\n",
    "  \n",
    "  # Training computation.\n",
    "  hidden_layer = tf.add(tf.matmul(tf_train_dataset, tf.cast(weights[0], tf.float32)), biases[0])\n",
    "  hidden_layer = tf.nn.relu(hidden_layer)\n",
    "  logits = tf.add(tf.matmul(hidden_layer, tf.cast(weights[1], tf.float32)), biases[1])\n",
    "#   logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_train_labels))\n",
    "  l2Loss = tf.nn.l2_loss(weights[0]) + tf.nn.l2_loss(weights[1])\n",
    "  loss = loss + 5*l2Loss\n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.001).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "    \n",
    "  # ValidPrediction\n",
    "#   temp1 = tf.add(tf.matmul(tf_valid_dataset, weights[0]), biases[0])\n",
    "#   hidden_temp = tf.nn.relu(temp1)\n",
    "#   temp2 = tf.add(tf.matmul(hidden_temp,weights[1]), biases[1])\n",
    "#   valid_prediction = tf.nn.softmax(temp2)\n",
    "\n",
    "  # TestPrediction\n",
    "  temp3 = tf.add(tf.matmul(tf.cast(tf_test_dataset, tf.float32), weights[0]), biases[0])\n",
    "  hidden_temp2 = tf.nn.relu(temp3)\n",
    "  temp4 = tf.add(tf.matmul(tf.cast(hidden_temp2, tf.float32), weights[1]), biases[1])\n",
    "  test_prediction = tf.nn.softmax(temp4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 249257.203125\n",
      "Minibatch accuracy: 38.0%\n",
      "Test accuracy: 78.5%\n",
      "Minibatch loss at step 300: 12029.495117\n",
      "Minibatch accuracy: 90.0%\n",
      "Test accuracy: 87.3%\n",
      "Minibatch loss at step 600: 592.334595\n",
      "Minibatch accuracy: 93.5%\n",
      "Test accuracy: 92.8%\n",
      "Minibatch loss at step 900: 29.545288\n",
      "Minibatch accuracy: 91.5%\n",
      "Test accuracy: 95.0%\n",
      "Minibatch loss at step 1200: 1.924597\n",
      "Minibatch accuracy: 90.0%\n",
      "Test accuracy: 94.7%\n",
      "Minibatch loss at step 1500: 0.602791\n",
      "Minibatch accuracy: 87.5%\n",
      "Test accuracy: 95.4%\n",
      "Minibatch loss at step 1800: 0.432140\n",
      "Minibatch accuracy: 90.5%\n",
      "Test accuracy: 95.0%\n",
      "Minibatch loss at step 2100: 0.496623\n",
      "Minibatch accuracy: 92.0%\n",
      "Test accuracy: 95.0%\n",
      "Minibatch loss at step 2400: 0.487768\n",
      "Minibatch accuracy: 89.0%\n",
      "Test accuracy: 94.9%\n",
      "Minibatch loss at step 2700: 0.437116\n",
      "Minibatch accuracy: 93.0%\n",
      "Test accuracy: 95.1%\n",
      "Minibatch loss at step 3000: 0.543201\n",
      "Minibatch accuracy: 88.0%\n",
      "Test accuracy: 95.0%\n",
      "Minibatch loss at step 3300: 0.495481\n",
      "Minibatch accuracy: 90.0%\n",
      "Test accuracy: 94.9%\n",
      "Minibatch loss at step 3600: 0.436145\n",
      "Minibatch accuracy: 90.0%\n",
      "Test accuracy: 94.8%\n",
      "Minibatch loss at step 3900: 0.535537\n",
      "Minibatch accuracy: 87.5%\n",
      "Test accuracy: 94.9%\n",
      "Minibatch loss at step 4200: 0.563649\n",
      "Minibatch accuracy: 87.5%\n",
      "Test accuracy: 95.1%\n",
      "Minibatch loss at step 4500: 0.540973\n",
      "Minibatch accuracy: 85.5%\n",
      "Test accuracy: 94.9%\n",
      "Minibatch loss at step 4800: 0.389926\n",
      "Minibatch accuracy: 94.0%\n",
      "Test accuracy: 95.0%\n",
      "Minibatch loss at step 5100: 0.413348\n",
      "Minibatch accuracy: 92.5%\n",
      "Test accuracy: 95.1%\n",
      "Minibatch loss at step 5400: 0.501673\n",
      "Minibatch accuracy: 88.5%\n",
      "Test accuracy: 94.9%\n",
      "Minibatch loss at step 5700: 0.447738\n",
      "Minibatch accuracy: 89.0%\n",
      "Test accuracy: 94.9%\n",
      "Minibatch loss at step 6000: 0.548643\n",
      "Minibatch accuracy: 90.0%\n",
      "Test accuracy: 95.2%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 6001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  #tf.initialize_all_variables().run()\n",
    "  tf.global_variables_initializer().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (sub_train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = sub_train_dataset.iloc[offset:(offset + batch_size), :].values[:,:]\n",
    "    batch_labels = sub_train_labels.iloc[offset:(offset + batch_size), :].values[:,:]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 300 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "#       print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "#         valid_prediction.eval(), valid_labels))\n",
    "      print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), sub_test_labels.values[:,:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model2: CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11948,
     "status": "ok",
     "timestamp": 1446658914837,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "016b1a51-0290-4b08-efdb-8c95ffc3cd01"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Training set', (200000, 28, 28), (200000,))\n",
      "('Validation set', (10000, 28, 28), (10000,))\n",
      "('Test set', (10000, 28, 28), (10000,))\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a TensorFlow-friendly shape:\n",
    "- convolutions need the image data formatted as a cube (width by height by #channels)\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11952,
     "status": "ok",
     "timestamp": 1446658914857,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "650a208c-8359-4852-f4f5-8bf10e80ef6c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Training set', (200000, 28, 28, 1), (200000, 10))\n",
      "('Validation set', (10000, 28, 28, 1), (10000, 10))\n",
      "('Test set', (10000, 28, 28, 1), (10000, 10))\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "num_channels = 1 # grayscale\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape(\n",
    "    (-1, image_size, image_size, num_channels)).astype(np.float32)\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "AgQDIREv02p1"
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5rhgjmROXu2O"
   },
   "source": [
    "Let's build a small network with two convolutional layers, followed by one fully connected layer. Convolutional networks are more expensive computationally, so we'll limit its depth and number of fully connected nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "IZYv70SvvOan"
   },
   "outputs": [],
   "source": [
    "batch_size = 16 #一次训练16个样本吗？不太清楚  bath_size上的strides=1 应该是一次处理一个样本\n",
    "patch_size = 5 #相当于是kernel的大小\n",
    "depth = 16 #最后输出的大小\n",
    "num_hidden = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data.\n",
    "  tf_train_dataset = tf.placeholder(\n",
    "    tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  # kernel里面的值随机初始化吗？？？\n",
    "  layer1_weights = tf.Variable(tf.truncated_normal(\n",
    "      [patch_size, patch_size, num_channels, depth], stddev=0.1))\n",
    "  layer1_biases = tf.Variable(tf.zeros([depth])) # biases初始化为0\n",
    "  layer2_weights = tf.Variable(tf.truncated_normal(\n",
    "      [patch_size, patch_size, depth, depth], stddev=0.1))\n",
    "  layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth])) # 下一次biases又变成1？？？\n",
    "  layer3_weights = tf.Variable(tf.truncated_normal(\n",
    "      [image_size // 4 * image_size // 4 * depth, num_hidden], stddev=0.1))\n",
    "  layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n",
    "  layer4_weights = tf.Variable(tf.truncated_normal(\n",
    "      [num_hidden, num_labels], stddev=0.1))\n",
    "  layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "  \n",
    "  # Model.\n",
    "  def model(data):\n",
    "    \"\"\"\n",
    "    ninglee 标注\n",
    "    conv2d(\n",
    "        input,\n",
    "        filter,\n",
    "        strides,\n",
    "        padding,\n",
    "        use_cudnn_on_gpu=None,\n",
    "        data_format=None, 表示输入输出的格式，默认[batch, height, width, channels]\n",
    "        name=None\n",
    "    )\n",
    "    input: 4-D [batch, in_height, in_width, in_channels]\n",
    "    filter/kernel: 4-D [filter_height, filter_width, in_channels, out_channels]\n",
    "    strides: 4-D 4个维度的步幅\n",
    "    padding: string \"SAME\" \"VALID\"\n",
    "    \"\"\"\n",
    "    # 第一层卷积变换\n",
    "    conv = tf.nn.conv2d(data, layer1_weights, [1, 2, 2, 1], padding='SAME') #strides = 2 ninglee标注\n",
    "    hidden = tf.nn.relu(conv + layer1_biases)# 卷积完了之后还要加上biases然后经过relu激活函数处理才算完\n",
    "    # 第二层卷积变换\n",
    "    conv = tf.nn.conv2d(hidden, layer2_weights, [1, 2, 2, 1], padding='SAME')\n",
    "    hidden = tf.nn.relu(conv + layer2_biases)\n",
    "    # 全连接层，同样也要经过relu激活\n",
    "    shape = hidden.get_shape().as_list()\n",
    "    reshape = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "    hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)\n",
    "    # 最后的输出层，使用softmax激活函数\n",
    "    return tf.matmul(hidden, layer4_weights) + layer4_biases\n",
    "  \n",
    "  # Training computation.\n",
    "  logits = model(tf_train_dataset)\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_train_labels))\n",
    "    \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n",
    "  test_prediction = tf.nn.softmax(model(tf_test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 37
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 63292,
     "status": "ok",
     "timestamp": 1446658966251,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "noKFb2UovVFR",
    "outputId": "28941338-2ef9-4088-8bd1-44295661e628"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 3.702314\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 9.9%\n",
      "Minibatch loss at step 50: 2.009227\n",
      "Minibatch accuracy: 25.0%\n",
      "Validation accuracy: 50.8%\n",
      "Minibatch loss at step 100: 1.051008\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy: 64.7%\n",
      "Minibatch loss at step 150: 0.693865\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 74.9%\n",
      "Minibatch loss at step 200: 1.195843\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 77.5%\n",
      "Minibatch loss at step 250: 0.924027\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 79.3%\n",
      "Minibatch loss at step 300: 0.626783\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 80.1%\n",
      "Minibatch loss at step 350: 1.750046\n",
      "Minibatch accuracy: 43.8%\n",
      "Validation accuracy: 79.4%\n",
      "Minibatch loss at step 400: 1.061372\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 80.3%\n",
      "Minibatch loss at step 450: 0.848145\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 80.2%\n",
      "Minibatch loss at step 500: 0.397961\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 81.0%\n",
      "Minibatch loss at step 550: 0.598633\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 81.6%\n",
      "Minibatch loss at step 600: 0.289897\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 82.1%\n",
      "Minibatch loss at step 650: 0.546314\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 81.6%\n",
      "Minibatch loss at step 700: 0.648622\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 82.8%\n",
      "Minibatch loss at step 750: 0.474605\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 83.5%\n",
      "Minibatch loss at step 800: 0.692389\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 82.4%\n",
      "Minibatch loss at step 850: 0.760631\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 82.5%\n",
      "Minibatch loss at step 900: 0.686464\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 83.8%\n",
      "Minibatch loss at step 950: 0.714912\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 83.5%\n",
      "Minibatch loss at step 1000: 0.637224\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 83.7%\n",
      "Test accuracy: 90.2%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 1001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  for step in range(num_steps):\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 50 == 0):\n",
    "      print('Minibatch loss at step %d: %f' % (step, l))\n",
    "      print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "      print('Validation accuracy: %.1f%%' % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KedKkn4EutIK"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "The convolutional model above uses convolutions with stride 2 to reduce the dimensionality. Replace the strides by a max pooling operation (`nn.max_pool()`) of stride 2 and kernel size 2.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 16 #一次训练16个样本吗？不太清楚  bath_size上的strides=1 应该是一次处理一个样本\n",
    "patch_size = 5 #相当于是kernel的大小\n",
    "depth = 16 #最后输出的大小\n",
    "num_hidden = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data.\n",
    "  tf_train_dataset = tf.placeholder(\n",
    "    tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  # kernel里面的值随机初始化吗？？？\n",
    "  layer1_weights = tf.Variable(tf.truncated_normal(\n",
    "      [patch_size, patch_size, num_channels, depth], stddev=0.1))\n",
    "  layer1_biases = tf.Variable(tf.zeros([depth])) # biases初始化为0\n",
    "  layer2_weights = tf.Variable(tf.truncated_normal(\n",
    "      [patch_size, patch_size, depth, depth], stddev=0.1))\n",
    "  layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth])) # 下一次biases又变成1？？？\n",
    "  layer3_weights = tf.Variable(tf.truncated_normal(\n",
    "      [image_size // 4 * image_size // 4 * depth, num_hidden], stddev=0.1))\n",
    "  layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n",
    "  layer4_weights = tf.Variable(tf.truncated_normal(\n",
    "      [num_hidden, num_labels], stddev=0.1))\n",
    "  layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "  \n",
    "  # Model.\n",
    "  def model(data):\n",
    "    \"\"\"\n",
    "    ninglee 标注\n",
    "    conv2d(\n",
    "        input,\n",
    "        filter,\n",
    "        strides,\n",
    "        padding,\n",
    "        use_cudnn_on_gpu=None,\n",
    "        data_format=None, 表示输入输出的格式，默认[batch, height, width, channels] NHWC\n",
    "        name=None\n",
    "    )\n",
    "    input: 4-D [batch, in_height, in_width, in_channels]\n",
    "    filter/kernel: 4-D [filter_height, filter_width, in_channels, out_channels]\n",
    "    strides: 4-D 4个维度的步幅\n",
    "    padding: string \"SAME\" \"VALID\"\n",
    "    \n",
    "    max_pool(\n",
    "        value,\n",
    "        ksize,\n",
    "        strides,\n",
    "        padding,\n",
    "        data_format='NHWC',\n",
    "        name=None\n",
    "    )\n",
    "    value: 4-D [batch, height, width, channels]\n",
    "    ksize: list of ints, 长度至少为4\n",
    "    strides: list of ints, 长度至少为4\n",
    "    padding: string, \"VALID\", \"SAME\"\n",
    "    \n",
    "    \"\"\"\n",
    "    # 第一层卷积变换\n",
    "    conv = tf.nn.conv2d(data, layer1_weights, [1, 1, 1, 1], padding='SAME') \n",
    "    hidden = tf.nn.relu(conv + layer1_biases)\n",
    "    hidden = tf.nn.max_pool(hidden, [1,2,2,1], [1,2,2,1], padding='SAME')\n",
    "    # 第二层卷积变换\n",
    "    conv = tf.nn.conv2d(hidden, layer2_weights, [1, 1, 1, 1], padding='SAME')\n",
    "    hidden = tf.nn.relu(conv + layer2_biases)\n",
    "    # 注意：一定是等到relu激活函数之后再池化\n",
    "    hidden = tf.nn.max_pool(hidden, [1,2,2,1], [1,2,2,1], padding='SAME')\n",
    "    # 全连接层，同样也要经过relu激活\n",
    "    shape = hidden.get_shape().as_list()\n",
    "    reshape = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "    hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)\n",
    "    # 最后的输出层，使用softmax激活函数\n",
    "    return tf.matmul(hidden, layer4_weights) + layer4_biases\n",
    "  \n",
    "  # Training computation.\n",
    "  logits = model(tf_train_dataset)\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_train_labels))\n",
    "    \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n",
    "  test_prediction = tf.nn.softmax(model(tf_test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/ninglee/anaconda/lib/python2.7/site-packages/tensorflow/python/util/tf_should_use.py:170: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Initialized\n",
      "Minibatch loss at step 0: 3.140602\n",
      "Minibatch accuracy: 6.2%\n",
      "Validation accuracy: 9.1%\n",
      "Minibatch loss at step 50: 2.370754\n",
      "Minibatch accuracy: 12.5%\n",
      "Validation accuracy: 23.0%\n",
      "Minibatch loss at step 100: 1.429823\n",
      "Minibatch accuracy: 43.8%\n",
      "Validation accuracy: 41.6%\n",
      "Minibatch loss at step 150: 0.721467\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 71.3%\n",
      "Minibatch loss at step 200: 1.137816\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy: 76.9%\n",
      "Minibatch loss at step 250: 0.949122\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 79.8%\n",
      "Minibatch loss at step 300: 0.557121\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 80.6%\n",
      "Minibatch loss at step 350: 1.618942\n",
      "Minibatch accuracy: 50.0%\n",
      "Validation accuracy: 79.9%\n",
      "Minibatch loss at step 400: 0.677687\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 80.4%\n",
      "Minibatch loss at step 450: 0.773237\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 81.4%\n",
      "Minibatch loss at step 500: 0.388165\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 81.4%\n",
      "Minibatch loss at step 550: 0.443717\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 82.4%\n",
      "Minibatch loss at step 600: 0.298772\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 82.6%\n",
      "Minibatch loss at step 650: 0.303113\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 83.0%\n",
      "Minibatch loss at step 700: 0.581118\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 83.8%\n",
      "Minibatch loss at step 750: 0.443888\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 84.8%\n",
      "Minibatch loss at step 800: 0.453447\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 83.2%\n",
      "Minibatch loss at step 850: 0.829193\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 84.1%\n",
      "Minibatch loss at step 900: 0.789914\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 83.8%\n",
      "Minibatch loss at step 950: 0.466448\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 84.1%\n",
      "Minibatch loss at step 1000: 0.557844\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 85.0%\n",
      "Test accuracy: 91.5%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 1001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  for step in range(num_steps):\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 50 == 0):\n",
    "      print('Minibatch loss at step %d: %f' % (step, l))\n",
    "      print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "      print('Validation accuracy: %.1f%%' % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "klf21gpbAgb-"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "Try to get the best performance you can using a convolutional net. Look for example at the classic [LeNet5](http://yann.lecun.com/exdb/lenet/) architecture, adding Dropout, and/or adding learning rate decay.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 16*4 #一次训练16个样本吗？不太清楚  bath_size上的strides=1 应该是一次处理一个样本\n",
    "patch_size = 5  #相当于是kernel的大小\n",
    "depth = 16 #最后输出的大小\n",
    "num_hidden = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data.\n",
    "  tf_train_dataset = tf.placeholder(\n",
    "    tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  layer1_weights = tf.Variable(tf.truncated_normal(\n",
    "      [patch_size, patch_size, num_channels, depth], stddev=0.1))\n",
    "  layer1_biases = tf.Variable(tf.zeros([depth])) # biases初始化为0\n",
    "  layer2_weights = tf.Variable(tf.truncated_normal(\n",
    "      [patch_size, patch_size, depth, depth], stddev=0.1))\n",
    "  layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth])) # 下一次biases又变成1？？？\n",
    "  layer3_weights = tf.Variable(tf.truncated_normal(\n",
    "      [image_size // 4 * image_size // 4 * depth, num_hidden], stddev=0.1))\n",
    "  layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n",
    "  layer4_weights = tf.Variable(tf.truncated_normal(\n",
    "      [num_hidden, num_labels], stddev=0.1))\n",
    "  layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "  \n",
    "  # Model.\n",
    "  def model(data):\n",
    "    # 第一层卷积变换\n",
    "    conv = tf.nn.conv2d(data, layer1_weights, [1, 1, 1, 1], padding='SAME') \n",
    "    hidden = tf.nn.relu(conv + layer1_biases)\n",
    "    hidden = tf.nn.max_pool(hidden, [1,2,2,1], [1,2,2,1], padding='SAME')\n",
    "    # 第二层卷积变换\n",
    "    conv = tf.nn.conv2d(hidden, layer2_weights, [1, 1, 1, 1], padding='SAME')\n",
    "    hidden = tf.nn.relu(conv + layer2_biases)\n",
    "    # 注意：一定是等到relu激活函数之后再池化\n",
    "    hidden = tf.nn.max_pool(hidden, [1,2,2,1], [1,2,2,1], padding='SAME')\n",
    "    # 全连接层，同样也要经过relu激活\n",
    "    shape = hidden.get_shape().as_list()\n",
    "    reshape = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "    hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)\n",
    "    # 最后的输出层，使用softmax激活函数\n",
    "    return tf.matmul(hidden, layer4_weights) + layer4_biases\n",
    "  \n",
    "  # Training computation.\n",
    "  logits = model(tf_train_dataset)\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_train_labels))\n",
    "#   l2_loss = tf.nn.l2_loss(layer1_weights) + tf.nn.l2_loss(layer2_weights) + tf.nn.l2_loss(layer3_weights)+ tf.nn.l2_loss(layer4_weights)\n",
    "  #loss = loss + 0.01*l2_loss\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0, trainable=False)  # count the number of steps taken.\n",
    "  learning_rate = tf.train.exponential_decay(0.1, global_step, 100,0.96,staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss,global_step=global_step)\n",
    "#   optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n",
    "  test_prediction = tf.nn.softmax(model(tf_test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/ninglee/anaconda/lib/python2.7/site-packages/tensorflow/python/util/tf_should_use.py:170: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Initialized\n",
      "Minibatch loss at step 0: 3.091755\n",
      "Minibatch accuracy: 12.5%\n",
      "Validation accuracy: 10.0%\n",
      "Minibatch loss at step 50: 1.000209\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy: 76.5%\n",
      "Minibatch loss at step 100: 0.642079\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 79.6%\n",
      "Minibatch loss at step 150: 0.576964\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 81.2%\n",
      "Minibatch loss at step 200: 0.840319\n",
      "Minibatch accuracy: 70.3%\n",
      "Validation accuracy: 81.9%\n",
      "Minibatch loss at step 250: 0.568166\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 83.3%\n",
      "Minibatch loss at step 300: 0.665424\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 84.1%\n",
      "Minibatch loss at step 350: 0.382421\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 84.4%\n",
      "Minibatch loss at step 400: 0.497308\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 84.8%\n",
      "Minibatch loss at step 450: 0.724603\n",
      "Minibatch accuracy: 76.6%\n",
      "Validation accuracy: 84.3%\n",
      "Minibatch loss at step 500: 0.573682\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 84.7%\n",
      "Minibatch loss at step 550: 0.635178\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 85.6%\n",
      "Minibatch loss at step 600: 0.502167\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 85.1%\n",
      "Minibatch loss at step 650: 0.724668\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 85.9%\n",
      "Minibatch loss at step 700: 0.439873\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 86.1%\n",
      "Minibatch loss at step 750: 0.485920\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 86.7%\n",
      "Minibatch loss at step 800: 0.380561\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 86.5%\n",
      "Minibatch loss at step 850: 0.199596\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 86.6%\n",
      "Minibatch loss at step 900: 0.357693\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 86.8%\n",
      "Minibatch loss at step 950: 0.719339\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 86.8%\n",
      "Minibatch loss at step 1000: 0.507728\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 87.3%\n",
      "Minibatch loss at step 1050: 0.493315\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 87.5%\n",
      "Minibatch loss at step 1100: 0.332303\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 87.5%\n",
      "Minibatch loss at step 1150: 0.686193\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 87.1%\n",
      "Minibatch loss at step 1200: 0.596366\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 87.2%\n",
      "Minibatch loss at step 1250: 0.399064\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 87.7%\n",
      "Minibatch loss at step 1300: 0.426133\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 88.0%\n",
      "Minibatch loss at step 1350: 0.431108\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 88.0%\n",
      "Minibatch loss at step 1400: 0.221825\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 88.0%\n",
      "Minibatch loss at step 1450: 0.477690\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 88.1%\n",
      "Minibatch loss at step 1500: 0.619354\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 87.8%\n",
      "Minibatch loss at step 1550: 0.244706\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 88.3%\n",
      "Minibatch loss at step 1600: 0.337363\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 88.0%\n",
      "Minibatch loss at step 1650: 0.506498\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 88.2%\n",
      "Minibatch loss at step 1700: 0.333230\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 88.2%\n",
      "Minibatch loss at step 1750: 0.327140\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 88.5%\n",
      "Minibatch loss at step 1800: 0.303447\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 88.4%\n",
      "Minibatch loss at step 1850: 0.449459\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 88.2%\n",
      "Minibatch loss at step 1900: 0.332284\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 88.3%\n",
      "Minibatch loss at step 1950: 0.301008\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 88.4%\n",
      "Minibatch loss at step 2000: 0.313316\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 88.6%\n",
      "Test accuracy: 94.4%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 2001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  for step in range(num_steps):\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 50 == 0):\n",
    "      print('Minibatch loss at step %d: %f' % (step, l))\n",
    "      print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "      print('Validation accuracy: %.1f%%' % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Result\n",
    "* start_learn_rate = 0.1\n",
    "* decay_step = 50\n",
    "* decay_rate = 0.96\n",
    "* l2_loss = False\n",
    "Minibatch loss at step 950: 0.574295\n",
    "Minibatch accuracy: 81.2%\n",
    "Validation accuracy: 86.2%\n",
    "Minibatch loss at step 1000: 0.738783\n",
    "Minibatch accuracy: 75.0%\n",
    "Validation accuracy: 86.3%\n",
    "Test accuracy: 92.4%\n",
    "\n",
    "* batch_size = 16 * 4\n",
    "* 还在长\n",
    "Minibatch loss at step 950: 0.728797\n",
    "Minibatch accuracy: 82.8%\n",
    "Validation accuracy: 87.0%\n",
    "Minibatch loss at step 1000: 0.506990\n",
    "Minibatch accuracy: 81.2%\n",
    "Validation accuracy: 87.3%\n",
    "Test accuracy: 93.1%\n",
    "\n",
    "* num_steps = 2001\n",
    "Minibatch loss at step 1950: 0.301008\n",
    "Minibatch accuracy: 87.5%\n",
    "Validation accuracy: 88.4%\n",
    "Minibatch loss at step 2000: 0.313316\n",
    "Minibatch accuracy: 87.5%\n",
    "Validation accuracy: 88.6%\n",
    "Test accuracy: 94.4%\n",
    "    \n",
    "* other ideas\n",
    "* 在全连层后面再增加一个全连接层，神经元数量设置为它的一半\n",
    "* 尝试把卷积映射的输出层数降低试试 depth\n",
    "* 卷积层增加一层？"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "4_convolutions.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "86px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
