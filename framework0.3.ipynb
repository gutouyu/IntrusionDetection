{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'autoreload 2'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Deep Learning\n",
    "# \n",
    "# ## preprocessing training dataset\n",
    "\n",
    "import os\n",
    "from MLP import MLP\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "u'matplotlib inline'\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "u'load_ext autoreload'\n",
    "u'autoreload 2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_data():\n",
    "    input_file_dir = \"../datasets\"\n",
    "    train_file_name = \"kddcup.data_10_percent.txt\"\n",
    "    test_file_name = \"corrected.txt\"\n",
    "    header_file_name = \"header.txt\"\n",
    "    train_files = os.path.join(input_file_dir, train_file_name)\n",
    "    test_files = os.path.join(input_file_dir, test_file_name)\n",
    "    header_files = os.path.join(input_file_dir, header_file_name)\n",
    "    with open(header_files, 'r') as f:\n",
    "        header = f.readline().strip().split(',')\n",
    "    train_dataset = pd.read_csv(train_files)\n",
    "    test_dataset = pd.read_csv(test_files)\n",
    "    train_dataset.columns = header\n",
    "    test_dataset.columns = header\n",
    "    return train_dataset, test_dataset\n",
    "\n",
    "\n",
    "def labels_map(label):\n",
    "    label = str(label).split('.')[0]\n",
    "    \n",
    "    DOS = ['apache2', 'back', 'land', 'mailbomb', 'neptune', 'pod', \n",
    "                  'processtable', 'smurf', 'teardrop', 'udpstorm'] #DOS 10个\n",
    "    PROBE = ['ipsweep', 'mscan', 'nmap', 'portsweep', 'saint', 'satan'] #PROBE\n",
    "    U2R = ['buffer_overflow', 'httptunnel', 'loadmodule', 'perl', 'ps', 'rootkit', 'sqlattack', 'xterm'] #U2R\n",
    "    R2L = ['ftp_write', 'guess_passwd', 'imap', 'multihop', 'named', 'phf', \n",
    "           'sendmail', 'snmpgetattack', 'snmpguess', 'spy', 'warezclient', \n",
    "           'warezmaster', 'worm', 'xlock', 'xsnoop']#R2L\n",
    "        \n",
    "    if label == 'normal':\n",
    "        return 0\n",
    "    if label in PROBE:\n",
    "        return -1 \n",
    "    if label in DOS:\n",
    "        return DOS.index(label) + 1\n",
    "    if label in U2R:\n",
    "        return -1\n",
    "    if label in R2L:\n",
    "        return -1\n",
    "\n",
    "def filter_labels(dataset):\n",
    "    dataset['labels'] = dataset['labels'].apply(labels_map)\n",
    "    #看看要保留哪些数据\n",
    "    dataset = dataset[(dataset['labels']>-1)]\n",
    "    return dataset\n",
    "\n",
    "def split_valid_from_train(train_dataset, valid_size):\n",
    "    # Method 1\n",
    "    train_dataset, valid_dataset, _, _ = train_test_split(train_dataset, train_dataset['labels'], test_size=valid_size, random_state=None)\n",
    "    \n",
    "    # pandas中先重置index再打乱train. 否则只会调整各个行的顺序，而不会改变pandas的index\n",
    "    # 重置\n",
    "    train_dataset = train_dataset.reset_index(drop=True)\n",
    "    # 打乱\n",
    "    indexMask = np.arange(len(train_dataset))\n",
    "    for i in xrange(10):\n",
    "        np.random.shuffle(indexMask)\n",
    "    train_dataset = train_dataset.iloc[indexMask]\n",
    "\n",
    "    # Method 2\n",
    "    #获取验证集\n",
    "    # val_frac=0.25\n",
    "    # valid_dataset_neg = train_dataset[(train_dataset['labels']==0)].sample(frac=val_frac)\n",
    "    # valid_dataset_pos = train_dataset[(train_dataset['labels']==2)].sample(frac=val_frac)\n",
    "    # valid_dataset = pd.concat([valid_dataset_neg, valid_dataset_pos], axis=0)\n",
    "    # #train_dataset中分离出valid_dataset\n",
    "    # train_dataset = train_dataset.select(lambda x: x not in valid_dataset.index, axis=0)\n",
    "    return train_dataset, valid_dataset\n",
    "\n",
    "def combine_train_valid_test(trainDF, validDF, testDF):\n",
    "    all = pd.concat([trainDF, validDF, testDF], axis=0)\n",
    "    return all, (trainDF.shape[0], validDF.shape[0], testDF.shape[0])\n",
    "\n",
    "def process_data_features(all, handle_labels=True):\n",
    "    if handle_labels:\n",
    "        # 独热编码 labels\n",
    "        labels_dummies = pd.get_dummies(all['labels'], prefix='label')\n",
    "        all = pd.concat([all,labels_dummies], axis=1)\n",
    "        all = all.drop(['labels'], axis=1)\n",
    "\n",
    "    # 独热编码 protocol_type\n",
    "    protocal_type_dummies = pd.get_dummies(all.protocol_type, prefix='protocol_type')\n",
    "    all = pd.concat([all, protocal_type_dummies], axis=1)\n",
    "    all = all.drop(['protocol_type'], axis=1)\n",
    "\n",
    "    # 独热编码 flag\n",
    "    flag_dummies = pd.get_dummies(all.flag, prefix='flag')\n",
    "    all = pd.concat([all, flag_dummies], axis=1)\n",
    "    all = all.drop(['flag'], axis=1)\n",
    "\n",
    "    # 独热编码 Service 共有66个 暂时先去掉\n",
    "    # all.service.value_counts()\n",
    "    # service_dummies = pd.get_dummies(all.service, prefix='service')\n",
    "    # all = pd.concat([all, service_dummies], axis=1)\n",
    "    all = all.drop(['service'], axis=1)\n",
    "\n",
    "    # 去中心化 src_bytes, dst_bytes\n",
    "    all['src_bytes_norm'] = all.src_bytes - all.src_bytes.mean()\n",
    "    all['dst_bytes_norm'] = all.dst_bytes - all.dst_bytes.mean()\n",
    "    all = all.drop(['src_bytes'], axis=1)\n",
    "    all = all.drop(['dst_bytes'], axis=1)\n",
    "\n",
    "    return all.astype('float')\n",
    "\n",
    "def recover_data_after_process_features(comb, num_comb, labels_list=[0,1,2,3]):\n",
    "    #分离出Train Valid Test\n",
    "    train_dataset_size, valid_dataset_size, test_dataset_size = num_comb\n",
    "    sub_train_dataset = comb.iloc[:train_dataset_size, :].sample(frac=1)\n",
    "    sub_valid_dataset = comb.iloc[train_dataset_size: train_dataset_size+valid_dataset_size, :].sample(frac=1)\n",
    "    sub_test_dataset = comb.iloc[train_dataset_size+valid_dataset_size:, :].sample(frac=1)\n",
    "    # 分离出 label\n",
    "    total_labels = ['label_%d' % i for i in labels_list]\n",
    "    print total_labels\n",
    "    \n",
    "    sub_train_labels = sub_train_dataset[total_labels]\n",
    "    sub_valid_labels = sub_valid_dataset[total_labels]\n",
    "    sub_test_labels = sub_test_dataset[total_labels]\n",
    "    sub_train_dataset.drop(total_labels, axis=1, inplace=True)\n",
    "    sub_valid_dataset.drop(total_labels, axis=1, inplace=True)\n",
    "    sub_test_dataset.drop(total_labels, axis=1, inplace=True)\n",
    "    data = {\n",
    "        'X_train': sub_train_dataset.as_matrix(),\n",
    "        'y_train': sub_train_labels.as_matrix(),\n",
    "        'X_val': sub_valid_dataset.as_matrix(),\n",
    "        'y_val': sub_valid_labels.as_matrix(),\n",
    "        'X_test': sub_test_dataset.as_matrix(),\n",
    "        'y_test': sub_test_labels.as_matrix()\n",
    "    }\n",
    "    for k, v in data.iteritems():\n",
    "        print k, v.shape\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "def analysis_plot_loss_and_accuracy(model):\n",
    "    print('Mean Accuracy( -top5 ) of train, valid, test: %.2f%%, %.2f%%, %.2f%%') % (100*np.mean(model.train_acc_history[-5:]),100*np.mean(model.val_acc_history[-5:]), 100*np.mean(model.test_acc_history[-5]))\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(2,1,1)\n",
    "    ax.plot(range(len(model.loss_history)), model.loss_history, '-o')\n",
    "    plt.plot()\n",
    "\n",
    "    plt_len = len(model.train_acc_history)\n",
    "    plt.subplot(2,1,2)\n",
    "    plt.title('Accuracy')\n",
    "    plt.plot(model.train_acc_history, '-o', label='train')\n",
    "    plt.plot(model.test_acc_history, '-o', label='test')\n",
    "    plt.plot(model.val_acc_history, '-o', label='valid')\n",
    "    # plt.plot([90] * plt_len, 'k--')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.gcf().set_size_inches(15,9)\n",
    "    plt.show()\n",
    "\n",
    "def accuracy(predictions, labels):\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "            / predictions.shape[0])\n",
    "\n",
    "def analysis_confusion_matrix(X,y, model, header, verbose=True):\n",
    "    y_pred = model.predict(X)\n",
    "    m = len(header)\n",
    "    y_true = np.argmax(y, axis=1)\n",
    "    y_predIdx = np.argmax(y_pred, axis=1)\n",
    "    mat = np.mat(np.zeros((m,m)))\n",
    "    for real, pred in zip(y_true,y_predIdx):\n",
    "        mat[real,pred] = mat[real,pred] + 1\n",
    "    confMatrix = pd.DataFrame(data=mat, index= header, columns=header, dtype=int)\n",
    "    if verbose:\n",
    "        print ('Accuracy: %.2f%%' % accuracy(y_pred, y))\n",
    "#         print confMatrix\n",
    "    return confMatrix\n",
    "\n",
    "def analysis_precision_recall(confMat):\n",
    "    \"\"\"\n",
    "    recall: 是相对于原始样本的, 检测率\n",
    "    precision: 是相对于预测结果的, 精确率\n",
    "    \"\"\"\n",
    "    m = len(confMat)\n",
    "    recall = np.diag(confMat) / np.sum(confMat, axis=1) #检测率\n",
    "    precission = np.diag(confMat) / np.sum(confMat, axis=0) #精确率\n",
    "    ret = pd.concat([recall, precission], axis=1)\n",
    "    ret.columns=['recall', 'precission']\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Prepare Data\n",
    "trainDF, testDF = read_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainDF = filter_labels(trainDF)\n",
    "testDF = filter_labels(testDF)\n",
    "testDF = testDF[(testDF['labels'] != 1) & (testDF['labels'] != 4) & (testDF['labels']!=7 ) & (testDF['labels']<10)]\n",
    "trainDF, validDF = split_valid_from_train(trainDF, 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'SF': 281802, 'S0': 65148, 'REJ': 19055, 'RSTO': 395, 'RSTR': 90, 'S1': 45, 'S2': 11, 'S3': 5})\n",
      "Counter({'SF': 225663, 'REJ': 39640, 'S0': 17444, 'RSTO': 976, 'RSTR': 129, 'S1': 18, 'S2': 15, 'S3': 4, 'RSTOS0': 1})\n",
      "Counter({'icmp': 211929, 'tcp': 139419, 'udp': 15203})\n",
      "Counter({'icmp': 164556, 'tcp': 103226, 'udp': 16108})\n"
     ]
    }
   ],
   "source": [
    "print Counter(trainDF['flag'])\n",
    "print Counter(testDF['flag'])\n",
    "print Counter(trainDF['protocol_type'])\n",
    "print Counter(testDF['protocol_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# protocal_labelencoder = LabelEncoder()\n",
    "# flag_labelencoder = LabelEncoder()\n",
    "\n",
    "# testDF['protocol_type'] = protocal_labelencoder.fit_transform(testDF['protocol_type'])\n",
    "# testDF['flag'] = flag_labelencoder.fit_transform(testDF['flag']).astype(int)\n",
    "# testDF.drop('service', axis=1, inplace=True)\n",
    "\n",
    "# trainDF['protocol_type'] = protocal_labelencoder.transform(testDF['protocol_type'])\n",
    "# trainDF['flag'] = flag_labelencoder.transform(trainDF['flag']).astype(int)\n",
    "# trainDF.drop('service',axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 10000, 5: 10000, 8: 10000, 2: 1629, 9: 762, 6: 196, 3: 14})\n"
     ]
    }
   ],
   "source": [
    "mask_8 = trainDF['labels'] == 8\n",
    "mask_5 = trainDF['labels'] == 5\n",
    "mask_0 = trainDF['labels'] == 0\n",
    "trainDF = pd.concat([trainDF[mask_8].sample(n=10000), \n",
    "                     trainDF[mask_5].sample(n=10000),\n",
    "                     trainDF[mask_0].sample(n=10000),\n",
    "                    trainDF[trainDF['labels'].isin([2,9,6,3])]], axis=0)\n",
    "\n",
    "print Counter(trainDF['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 10000, 2: 6516, 3: 5012, 5: 10000, 6: 5096, 8: 10000, 9: 5334})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = Counter(trainDF['labels'])\n",
    "expect_sample_size = 5000\n",
    "for label in [2,9,6,3]:\n",
    "    copy_cnt = expect_sample_size // count[label]\n",
    "    copy_part = trainDF[trainDF['labels'] == label]\n",
    "    for i in xrange(copy_cnt):\n",
    "        trainDF = pd.concat([trainDF, copy_part], axis=0)\n",
    "Counter(trainDF['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "combine, num_combine = combine_train_valid_test(trainDF, validDF, testDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "combine = process_data_features(combine, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['label_0', 'label_2', 'label_3', 'label_5', 'label_6', 'label_8', 'label_9']\n",
      "X_val (122184, 51)\n",
      "X_train (51958, 51)\n",
      "X_test (283890, 51)\n",
      "y_val (122184, 7)\n",
      "y_train (51958, 7)\n",
      "y_test (283890, 7)\n"
     ]
    }
   ],
   "source": [
    "data = recover_data_after_process_features(combine, num_combine, [0,2,3,5,6,8,9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\xe8\\xbf\\x87\\xe9\\x87\\x87\\xe6\\xa0\\xb7'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"过采样\"\"\"\n",
    "# normal, back, land, neptune, pod, smurf, teardrop\n",
    "# DOS = np.array(DOS)[[0,2,3,5,6,8,9]].tolist()\n",
    "# sm = SMOTE(random_state=12)\n",
    "# X_train, y_train = sm.fit_sample(trainDF.drop('labels',axis=1), trainDF['labels'])\n",
    "# trainDF = pd.DataFrame(data=X_train, columns=trainDF.drop('labels',axis=1).columns)\n",
    "# y_train = pd.DataFrame(data=y_train, columns=['lables'])\n",
    "# trainDF = pd.concat([trainDF, y_train], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 0 / 2590) train acc: 28.00%; val_acc: 23.65%; test_acc: 44.55%\n",
      "(Iteration 200 / 2590) train acc: 83.50%; val_acc: 99.03%; test_acc: 99.07%\n",
      "(Iteration 400 / 2590) train acc: 96.00%; val_acc: 93.68%; test_acc: 93.88%\n",
      "(Iteration 600 / 2590) train acc: 97.50%; val_acc: 98.46%; test_acc: 98.76%\n",
      "(Iteration 800 / 2590) train acc: 78.50%; val_acc: 82.57%; test_acc: 77.87%\n",
      "(Iteration 1000 / 2590) train acc: 86.50%; val_acc: 98.65%; test_acc: 99.11%\n",
      "(Iteration 1200 / 2590) train acc: 89.50%; val_acc: 98.98%; test_acc: 96.26%\n",
      "(Iteration 1400 / 2590) train acc: 97.00%; val_acc: 98.77%; test_acc: 99.04%\n",
      "(Iteration 1600 / 2590) train acc: 94.00%; val_acc: 93.26%; test_acc: 92.49%\n",
      "(Iteration 1800 / 2590) train acc: 91.00%; val_acc: 99.12%; test_acc: 98.66%\n",
      "(Iteration 2000 / 2590) train acc: 97.00%; val_acc: 99.29%; test_acc: 99.20%\n",
      "(Iteration 2200 / 2590) train acc: 100.00%; val_acc: 98.34%; test_acc: 97.90%\n",
      "(Iteration 2400 / 2590) train acc: 98.00%; val_acc: 98.42%; test_acc: 98.30%\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "input_dim = data['X_train'].shape[1]\n",
    "output_dim = data['y_train'].shape[1]\n",
    "model = MLP(data, input_dim, [512],output_dim,\n",
    "                learning_rate=1e-4, #1e-6\n",
    "                dropout_prob=0.0,\n",
    "                l2_strength=0.1,\n",
    "                batch_size=200,\n",
    "                num_epochs=10,\n",
    "                print_every=200,\n",
    "                verbose=True)\n",
    "\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "     normal       1.00      0.93      0.96     60592\n",
      "       back       0.96      1.00      0.98      1098\n",
      "       land       0.02      0.78      0.03         9\n",
      "    neptune       1.00      0.99      0.99     58001\n",
      "        pod       0.16      0.91      0.27        87\n",
      "      smurf       1.00      1.00      1.00    164091\n",
      "   teardrop       0.00      0.75      0.00        12\n",
      "\n",
      "avg / total       1.00      0.98      0.99    283890\n",
      "\n",
      "Accuracy 98.25\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "DOS = ['normal', 'apache2', 'back', 'land', 'mailbomb', 'neptune', 'pod', \n",
    "                  'processtable', 'smurf', 'teardrop', 'udpstorm'] #DOS 10个\n",
    "DOS = np.array(DOS)[[0,2,3,5,6,8,9]].tolist()\n",
    "\n",
    "y_pred = model.predict(data['X_test'])\n",
    "print classification_report(np.argmax(data['y_test'],axis=1), np.argmax(y_pred, axis=1), target_names=DOS)\n",
    "print 'Accuracy %.2f' % accuracy(y_pred, data['y_test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 98.25%\n",
      "          normal  back  land  neptune  pod   smurf  teardrop\n",
      "normal     56339    44   432      210  425       0      3142\n",
      "back           4  1094     0        0    0       0         0\n",
      "land           0     0     7        0    0       0         2\n",
      "neptune        0     0     7    57519    0       0       475\n",
      "pod            6     0     0        0   79       0         2\n",
      "smurf        163     0     0        0    0  163879        49\n",
      "teardrop       0     0     3        0    0       0         9\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>recall</th>\n",
       "      <th>precission</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>normal</th>\n",
       "      <td>0.929809</td>\n",
       "      <td>0.996939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>back</th>\n",
       "      <td>0.996357</td>\n",
       "      <td>0.961336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>land</th>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.015590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neptune</th>\n",
       "      <td>0.991690</td>\n",
       "      <td>0.996362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pod</th>\n",
       "      <td>0.908046</td>\n",
       "      <td>0.156746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>smurf</th>\n",
       "      <td>0.998708</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>teardrop</th>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.002446</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            recall  precission\n",
       "normal    0.929809    0.996939\n",
       "back      0.996357    0.961336\n",
       "land      0.777778    0.015590\n",
       "neptune   0.991690    0.996362\n",
       "pod       0.908046    0.156746\n",
       "smurf     0.998708    1.000000\n",
       "teardrop  0.750000    0.002446"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Analysis\n",
    "DOS = ['normal', 'apache2', 'back', 'land', 'mailbomb', 'neptune', 'pod', \n",
    "                  'processtable', 'smurf', 'teardrop', 'udpstorm'] #DOS 10个\n",
    "DOS = np.array(DOS)[[0,2,3,5,6,8,9]].tolist()\n",
    "ATTACK = ['normal', 'probe', 'dos', 'u2r', 'r2l']\n",
    "\n",
    "confMat = analysis_confusion_matrix(data['X_test'],data['y_test'], model, DOS)\n",
    "print confMat\n",
    "\n",
    "analysis_precision_recall(confMat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy( -top5 ) of train, valid, test: 96.00%, 97.69%, 92.49%\n"
     ]
    }
   ],
   "source": [
    "analysis_plot_loss_and_accuracy(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
